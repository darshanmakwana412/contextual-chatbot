{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import rasa\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir, load_dataset_path, no_intents=None, entities=True):\n",
    "    with open(load_dataset_path, 'w') as yaml_file:\n",
    "        rasa_version = \"3.1\"\n",
    "        yaml_file.writelines(f\"version: {rasa_version}\")\n",
    "        yaml_file.write(\"\\n\\n\")\n",
    "        yaml_file.writelines(\"nlu:\\n\")\n",
    "        intents = os.listdir(dataset_dir)\n",
    "        for intent_file_name in intents:\n",
    "            intent_path = os.path.join(dataset_dir, intent_file_name)\n",
    "            intent_name = intent_file_name[:-5]\n",
    "            yaml_file.writelines(f\"- intent: {intent_name}\\n\")\n",
    "            yaml_file.writelines(\"  examples: |\\n\")\n",
    "            with open(intent_path) as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                intent_examples = json_data[intent_name]\n",
    "                if(no_intents==None):\n",
    "                    no_intents=len(intent_examples)\n",
    "                text_examples = []\n",
    "                intent_examples = random.sample(intent_examples, no_intents)\n",
    "                for data in intent_examples:\n",
    "                    texts = data[\"data\"]\n",
    "                    example = \"\"\n",
    "                    for text in texts:\n",
    "                        word = text[\"text\"]\n",
    "                        if entities:\n",
    "                            if \"entity\" in text.keys():\n",
    "                                entity = text[\"entity\"]\n",
    "                                word = f\"[{word}]({entity})\"\n",
    "                        example = example + word\n",
    "                    yaml_file.writelines(f\"    - {example}\\n\")\n",
    "            yaml_file.writelines(\"\\n\")\n",
    "            json_file.close()\n",
    "    yaml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dir = \"./dataset/Train/\"\n",
    "val_dataset_dir = \"./dataset/Validate/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(train_dataset_dir, \"./model_data/train_20_data.yml\", no_intents=20, entities=True)\n",
    "load_data(train_dataset_dir, \"./model_data/train_100_data.yml\", no_intents=100, entities=True)\n",
    "load_data(train_dataset_dir, \"./model_data/train_200_data.yml\", no_intents=200, entities=True)\n",
    "\n",
    "load_data(val_dataset_dir, \"./model_data/val_data_entities.yml\", entities=True)\n",
    "load_data(val_dataset_dir, \"./model_data/val_data_no_entities.yml\", entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(train_dataset_dir, \"./model_data/train_20_data.yml\", no_intents=20, entities=True)\n",
    "load_data(val_dataset_dir, \"./model_data/val_data_entities.yml\", entities=True)\n",
    "load_data(val_dataset_dir, \"./model_data/val_data_no_entities.yml\", entities=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa import model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mc:\\Darshan\\contextual-chatbot\\env\\lib\\site-packages\\rasa\\shared\\utils\\io.py:98: UserWarning: The out of vocabulary token 'oov' was configured, but could not be found in any one of the NLU training examples. All unseen words will be ignored during prediction.\n",
      "  More info at https://rasa.com/docs/rasa/components#countvectorsfeaturizer\n",
      "\u001b[0mc:\\Darshan\\contextual-chatbot\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Rasa model is trained and saved at 'models\\nlu-20221220-110132-fast-regression.tar.gz'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models\\\\nlu-20221220-110132-fast-regression.tar.gz'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rasa.model_training.train_nlu(\"./configs/config-origin.yml\", \"./model_data/train_20_data.yml\", \"./models/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasa.core.agent import Agent\n",
    "\n",
    "model_name = \"spacy-nlp\"\n",
    "\n",
    "model_path = f\"./models/{model_name}.gz\"\n",
    "nlu_agent = Agent.load(model_path=model_path)\n",
    "\n",
    "def get_prediction(query):\n",
    "    pred = asyncio.run(nlu_agent.parse_message(message_data=query))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rasa.nlu.extractors.crf_entity_extractor' from 'c:\\\\Darshan\\\\contextual-chatbot\\\\env\\\\lib\\\\site-packages\\\\rasa\\\\nlu\\\\extractors\\\\crf_entity_extractor.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rasa.nlu.extractors.crf_entity_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'add the artist Pete Murray to my relaxing playlist',\n",
       " 'intent': {'name': 'AddToPlaylist', 'confidence': 0.997940942789076},\n",
       " 'entities': [{'entity': 'music_item',\n",
       "   'start': 8,\n",
       "   'end': 14,\n",
       "   'confidence_entity': 0.7597190824015085,\n",
       "   'value': 'artist',\n",
       "   'extractor': 'CRFEntityExtractor'},\n",
       "  {'entity': 'playlist_owner',\n",
       "   'start': 30,\n",
       "   'end': 32,\n",
       "   'confidence_entity': 0.8044529980000107,\n",
       "   'value': 'my',\n",
       "   'extractor': 'CRFEntityExtractor'}],\n",
       " 'text_tokens': [(0, 3),\n",
       "  (4, 7),\n",
       "  (8, 14),\n",
       "  (15, 19),\n",
       "  (20, 26),\n",
       "  (27, 29),\n",
       "  (30, 32),\n",
       "  (33, 41),\n",
       "  (42, 50)],\n",
       " 'intent_ranking': [{'name': 'AddToPlaylist', 'confidence': 0.997940942789076},\n",
       "  {'name': 'SearchCreativeWork', 'confidence': 0.0009855166099354347},\n",
       "  {'name': 'PlayMusic', 'confidence': 0.0008531223411077812},\n",
       "  {'name': 'SearchScreeningEvent', 'confidence': 9.37053682549645e-05},\n",
       "  {'name': 'RateBook', 'confidence': 6.285805722360564e-05}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"add the artist Pete Murray to my relaxing playlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasa.shared.nlu.training_data.loading as nlu_loading\n",
    "\n",
    "val_data = nlu_loading.load_data(\"./model_data/val_data_no_entities.yml\")\n",
    "val_data = [m.as_dict() for m in val_data.intent_examples]\n",
    "\n",
    "val_data_entity = nlu_loading.load_data(\"./model_data/val_data_entities.yml\")\n",
    "val_data_entity = [m.as_dict() for m in val_data_entity.entity_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Query\", \"Expected Entity\", \"Predicted Entity\", \"Confidence\"]\n",
    "result = pd.DataFrame([], columns=columns)\n",
    "\n",
    "for i in range(len(val_data)):\n",
    "    query = val_data[i][\"text\"]\n",
    "    pred = get_prediction(query)\n",
    "    pred = pred[\"entities\"]\n",
    "    expec = val_data_entity[i][\"entities\"]\n",
    "    token_point = {}\n",
    "    for pred_entity in pred:\n",
    "        token_point[pred_entity[\"start\"]] = {\n",
    "            \"Expected Entity\": \"\"\n",
    "        }\n",
    "        token_point[pred_entity[\"start\"]][\"Predicted Entity\"] = pred_entity[\"entity\"]\n",
    "        token_point[pred_entity[\"start\"]][\"Confidence\"] = pred_entity[\"confidence_entity\"]\n",
    "    for expec_entity in expec:\n",
    "        if expec_entity[\"start\"] not in token_point:\n",
    "            token_point[expec_entity[\"start\"]] = {\n",
    "                \"Predicted Entity\": \"\",\n",
    "                \"Confidence\": 0,\n",
    "            }\n",
    "        token_point[expec_entity[\"start\"]][\"Expected Entity\"] = expec_entity[\"entity\"]\n",
    "\n",
    "    for token in token_point.values():\n",
    "        row= {}\n",
    "        row[\"Query\"] = query\n",
    "        row[\"Expected Entity\"] = token[\"Expected Entity\"]\n",
    "        row[\"Predicted Entity\"] = token[\"Predicted Entity\"]\n",
    "        row[\"Confidence\"] = token[\"Confidence\"]\n",
    "        result = pd.concat([result, pd.DataFrame(row, index=[0])], ignore_index=True)\n",
    "\n",
    "result.to_csv(f\"./logs/entity-{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'play some music by imagine dragons',\n",
       " 'intent': {'name': 'PlayMusic', 'confidence': 0.9769954244727165},\n",
       " 'entities': [],\n",
       " 'text_tokens': [(0, 4), (5, 9), (10, 15), (16, 18), (19, 26), (27, 34)],\n",
       " 'intent_ranking': [{'name': 'PlayMusic', 'confidence': 0.9769954244727165},\n",
       "  {'name': 'SearchScreeningEvent', 'confidence': 0.008738545910329966},\n",
       "  {'name': 'SearchCreativeWork', 'confidence': 0.0072978377319007975},\n",
       "  {'name': 'AddToPlaylist', 'confidence': 0.004068762565745417},\n",
       "  {'name': 'RateBook', 'confidence': 0.0013848580976571585}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"play some music by imagine dragons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Query\", \"Expected Intent\", \"Predicted Intent\"]+[intent[:-5] for intent in os.listdir(\"./dataset/Train/\")]\n",
    "result = pd.DataFrame([], columns=columns)\n",
    "\n",
    "true_positive = 0\n",
    "\n",
    "for data in val_data:\n",
    "    row = {}\n",
    "    query = data[\"text\"]\n",
    "    expected_intent = data[\"intent\"]\n",
    "    row[\"Query\"] = query\n",
    "    row[\"Expected Intent\"] = expected_intent\n",
    "    pred = get_prediction(query)\n",
    "    row[\"Predicted Intent\"] = pred[\"intent\"][\"name\"]\n",
    "    if row[\"Predicted Intent\"] == row[\"Expected Intent\"]:\n",
    "        true_positive += 1\n",
    "    for pred_intent in pred[\"intent_ranking\"]:\n",
    "        row[pred_intent[\"name\"]] = pred_intent[\"confidence\"]\n",
    "\n",
    "    result = pd.concat([result, pd.DataFrame(row, index=[0])], ignore_index=True)\n",
    "\n",
    "result.to_csv(f\"./logs/intent-{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8852223816355811\n"
     ]
    }
   ],
   "source": [
    "print(true_positive/len(val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0c32a0b60d0648371e98da90f122043fd757435ec9f16445842891d61e2681"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
